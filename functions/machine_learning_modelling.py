# -*- coding: utf-8 -*-
"""Machine Learning Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eAGEnV5r3INxcq1pZHo-xza3EdbBmkS5
"""

# Commented out IPython magic to ensure Python compatibility.
# import the necessary libraries
# %load_ext autoreload
# %autoreload 2
from functions.Data_Cleaning_Function import *
from functions.Data_Vectorization_Function import *
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt

"""#Load in Data 

Load in the vectorized data to run machine learning algorithm on. 
"""

# load in raw data csv's updated with row containing number labels

#'2016datafilepath' is a placeholder for the path to the 2016updated CSV file as it appears on your computer
df2016Updated = pd.read_csv('2016datafilepath.csv')

#'2017datafilepath' is a placeholder for the path to the 2017updated CSV file as it appears on your computer
df2017Updated = pd.read_csv('2017datafilepath.csv')

#'2018datafilepath' is a placeholder for the path to the 2018updated CSV file as it appears on your computer
df2018Updated = pd.read_csv('2018datafilepath.csv')

#'2019datafilepath' is a placeholder for the path to the 2019updated CSV file as it appears on your computer
df2019Updated = pd.read_csv('2019datafilepath.csv')

#'2020datafilepath' is a placeholder for the path to the 2020updated CSV file as it appears on your computer
df2020Updated = pd.read_csv('2020datafilepath.csv')

#'2021datafilepath' is a placeholder for the path to the 2021updated CSV file as it appears on your computer
df2021Updated = pd.read_csv('2021datafilepath.csv')

dfCombinedText = makeTextMatrix(df2016Updated, df2017Updated, df2018Updated, df2019Updated, df2020Updated, df2021Updated)
dfCombinedVectorized = vectorizeTextMatrix(dfCombinedText)

"""# Split data into testing set and trainng set

Split the data into testing and training set with the 80% for the train data and 20% for the test data
"""

# split the data into training and testing sets
xTrain, xTest, yTrain, yTest = train_test_split(dfCombinedVectorized.drop('45', axis=1), dfCombinedVectorized['45'], test_size=0.2)

"""# Logistic Regression

We build logistic regression model on the train data. Then, we evaluate the model performance using the test data.
"""

# create a logistic regression object
logReg = LogisticRegression()

# train the model on the training data
logReg.fit(xTrain, yTrain)

# make predictions on the testing data
predictions = logReg.predict(xTest)

# evaluate the model performance
accuracy = accuracy_score(yTest, predictions)
confusionMatrix = confusion_matrix(yTest, predictions)

print("Accuracy:", accuracy)
print("Confusion matrix:", confusionMatrix)

"""# Random Forest

We build random forest model on the train data. We using the out of bag error rate to tune the best number of tree. Then, we evaluate the model performance using the test data. 
"""

#Tune random forest classifier number of trees by plotting a oob error rate per tree number
from collections import OrderedDict
ensemble_clfs = [
    (
        "RandomForestClassifier",
        RandomForestClassifier(
            warm_start=True,
            oob_score=True,
        ),
    ),
]

# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.
error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)

# Range of `n_estimators` values to explore.
min_estimators = 2
max_estimators = 50

for label, clf in ensemble_clfs:
    for i in range(min_estimators, max_estimators+1 , 2):
        #print(i)
        clf.set_params(n_estimators=i)
        clf.fit(xTrain, yTrain)

        # Record the OOB error for each `n_estimators=i` setting.
        oob_error = 1 - clf.oob_score_
        #print(oob_error)
        error_rate[label].append((i, oob_error))
        

# Generate the "OOB error rate" vs. "n_estimators" plot.
for label, clf_err in error_rate.items():
    xs, ys = zip(*clf_err)
    plt.plot(xs, ys, label=label)


plt.xlabel("Number of trees")
plt.ylabel("OOB error rate")
plt.legend(loc="upper right")
plt.show()

# create a random forest object
rf = RandomForestClassifier(n_estimators=41)

# train the model on the training data
rf.fit(xTrain, yTrain)

# make predictions on the testing data
predictions = rf.predict(xTest)

# evaluate the model performance
accuracy = accuracy_score(yTest, predictions)
confusionMatrixRandomForest = confusion_matrix(yTest, predictions)

print("Accuracy:", accuracy)
print("Confusion matrix:", confusionMatrixRandomForest)

"""# Decision Tree

We build a decision tree model on the train data. Then, we evaluate the model performance using the test data. 
"""

# create a decision tree object
dt = DecisionTreeClassifier()

# train the model on the training data
dt.fit(xTrain, yTrain)

# make predictions on the testing data
predictions = dt.predict(xTest)

# evaluate the model performance
accuracy = accuracy_score(yTest, predictions)
confusionMatrixDecisionTree = confusion_matrix(yTest, predictions)

print("Accuracy:", accuracy)
print("Confusion matrix:", confusionMatrixDecisionTree)

#Next Steps
#focus on model interepretability
#Random Forest: rank features in order of importance
#Association algorithms (apriori)
#SHAP

