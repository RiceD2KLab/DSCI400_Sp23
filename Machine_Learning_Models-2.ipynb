{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents:\n",
        "1. [Load in the Data](#first)\n",
        "2. [Create Vectorized Combined Dataframe](#second)\n",
        "3. [Chi Squared Test](#third)\n",
        "4. [Splitting the Dataset](#fourth)\n",
        "5. [Logistic Regression](#fifth)\n",
        "6. [Random Forest](#sixth)\n",
        "7. [Decision Tree](#seventh)\n",
        "8. [Comparing Model Results](#eigth)\n",
        "9. [Feature Importance](#ninth)"
      ],
      "metadata": {
        "id": "7DwlnIPuhL7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "-nBRm99Mh66V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assuming you are using Google Colab, running this cell will clone the entire repository into Colab\n",
        "!git clone https://github.com/RiceD2KLab/DSCI400_Sp23.git"
      ],
      "metadata": {
        "id": "X2ILlDpLh8bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Go to repository folder containing Jupyter notebooks and python files to run\n",
        "%cd DSCI400_Sp23"
      ],
      "metadata": {
        "id": "SRuyGlIPh9qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all functions defined for data cleaning and data vectorization\n",
        "from data_cleaning_functions import *\n",
        "from data_vectorization_function import *"
      ],
      "metadata": {
        "id": "F9PLr3fUh-91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CoTlTIfGDQM"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import resample\n",
        "import scipy.stats as stats\n",
        "import matplotlib.patches as mpatches\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load in the Data <a name=\"first\"></a>\n",
        "\n",
        "Before loading in our data, we wanted to find the shared questions (features) across all years. Using the 2016 csv OSMI dataset, we assigned an integer to each question. For each question from 2016, we went through the 2017-2021 datasets to find questions that asked the same things (even if the wording was slightly different) and assigned shared questions across all years the same integer. If a question did not appear in all years from 2016-2021 surveys, that question did not get assigned an integer. A row containing these integers was added as the first row in each year's dataset.\n"
      ],
      "metadata": {
        "id": "DKcbN1RoiHqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load in raw data csv's updated with row containing number labels\n",
        "#Download 2016-2021 csv files from data folder in github repository\n",
        "#Click on \"Files\" tab on left-hand side and upload 2016-2021 csv files \n",
        "\n",
        "df2016Updated = pd.read_csv('/content/2016updated.csv')\n",
        "df2017Updated = pd.read_csv('/content/2017updated.csv')\n",
        "df2018Updated = pd.read_csv('/content/2018updated.csv')\n",
        "df2019Updated = pd.read_csv('/content/2019updated.csv')\n",
        "df2020Updated = pd.read_csv('/content/2020updated.csv')\n",
        "df2021Updated = pd.read_csv('/content/2021updated.csv')"
      ],
      "metadata": {
        "id": "A_EHn9GGiIyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Vectorized Combined Dataframe <a name=\"second\"></a>\n",
        "\n",
        "Use makeTextMatrix function defined in data_cleaning_functions python file and vectorizeDataframe function defined in data_vectorization_function file to create vectorized dataframes combining all years data between 2016-2021."
      ],
      "metadata": {
        "id": "1jhheyUNiXeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create combined text matrix of all 6 years\n",
        "dfCombined = makeTextMatrix(df2016Updated, df2017Updated, df2018Updated, df2019Updated, df2020Updated, df2021Updated)\n",
        "\n",
        "#Create precovid matrix concatenating 2016-2019 years' data\n",
        "dfPreCovid = makePreCovidMatrix(df2016Updated, df2017Updated, df2018Updated, df2019Updated)\n",
        "\n",
        "#Create during matrix concatenating 2020-2021 years' data\n",
        "dfDuringCovid = makeDuringCovidMatrix(df2020Updated, df2021Updated)\n",
        "\n",
        "#filter out features with more than 30% null values\n",
        "dfCombinedFinal, dfPreCovidFinal, dfDuringCovidFinal = filterNull(dfCombined, dfPreCovid, dfDuringCovid)\n",
        "\n",
        "#vectorize dfCombinedFinal\n",
        "dfCombinedVectorized = vectorizeDataframe(dfCombinedFinal)\n",
        "\n",
        "#drop features deemed unwanted from data exploration\n",
        "dfCombinedVectorized.drop(['51', '52', '1', '25'], axis=1, inplace=True)\n",
        "dfCombinedVectorized\n"
      ],
      "metadata": {
        "id": "l64LSz3Dkwk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chi-Squared Test <a name=\"third\"></a>\n",
        "\n",
        "We used the chi-squared test to identify features that have statistically significant associations with the output variable. We then selected those features as input variables for our machine learning models.\n",
        "\n"
      ],
      "metadata": {
        "id": "jMt3_E1mhFk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output variable and the input features\n",
        "\n",
        "outputVar = '45'\n",
        "inputVars = dfCombinedVectorized.drop('45', axis=1).columns\n",
        "\n",
        "# Calculate the contingency tables and the chi-squared statistic for each input variable\n",
        "for var in inputVars:\n",
        "    # Create a contingency table\n",
        "    contingencyTable = pd.crosstab(dfCombinedVectorized[outputVar], dfCombinedVectorized[var])\n",
        "    \n",
        "    # Calculate the chi-squared statistic and the p-value\n",
        "    chi2, p, _, _ = stats.chi2_contingency(contingencyTable)\n",
        "    \n",
        "    # Print the results\n",
        "    print(f'Chi-squared test for {var} and {outputVar}:')\n",
        "    print(f'Chi-squared statistic = {chi2:.4f}, p-value = {p:.4f}')"
      ],
      "metadata": {
        "id": "TEuG7BFpl5jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputVar = '45'\n",
        "inputVars = dfCombinedVectorized.drop(['45', '53'], axis=1).columns #drop output variable and age (continuous variable so chi-squared test won't apply)\n",
        "chiValues = []\n",
        "colors = []\n",
        "\n",
        "# Calculate the contingency tables and the chi-squared statistic for each input variable\n",
        "for var in inputVars:\n",
        "    # Create a contingency table\n",
        "    contingencyTable = pd.crosstab(dfCombinedVectorized[outputVar], dfCombinedVectorized[var])\n",
        "    \n",
        "    # Calculate the chi-squared statistic and the p-value\n",
        "    chi2, p, _, _ = stats.chi2_contingency(contingencyTable)\n",
        "    \n",
        "    # Append the chi-squared value to the list\n",
        "    chiValues.append(chi2)\n",
        "    \n",
        "    # Set the color of the bar based on the p-value\n",
        "    if p < 0.05:\n",
        "        colors.append('tab:blue')\n",
        "    else:\n",
        "        colors.append('tab:gray')\n",
        "\n",
        "# Sort the input variables in descending order of their corresponding chi-squared values\n",
        "inputVarsSorted, chiValuesSorted, colorsSorted = zip(*sorted(zip(inputVars, chiValues, colors), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# Create a bar plot of the chi-squared values\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "ax.bar(inputVarsSorted, chiValuesSorted, color=colorsSorted, hatch='/'*3, edgecolor='black')\n",
        "ax.set_xticklabels(inputVarsSorted, rotation=90)\n",
        "ax.set_ylabel('Chi-squared value')\n",
        "ax.set_xlabel('Question Indices')\n",
        "ax.set_title(f'Chi-squared values for \"Do you currently have a mental health disorder\" and input variables')\n",
        "\n",
        "# Create legend patches\n",
        "blue_patch = mpatches.Patch(color='tab:blue', label='Significant (p < 0.05)')\n",
        "gray_patch = mpatches.Patch(color='tab:gray', label='Not significant (p > 0.05)')\n",
        "\n",
        "# Add legend\n",
        "ax.legend(handles=[blue_patch, gray_patch])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TR4TIgjcl-nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecting features based on Chi-squared test\n",
        "dfFeatureSelected = dfCombinedVectorized.drop(['2', '30', '7', '9', '29', '14', '61', '3', '33'], axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "U1m1kHnTX7MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting the Dataset <a name=\"fourth\"></a>"
      ],
      "metadata": {
        "id": "WyWiiuCWTCuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into training and testing sets\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(dfFeatureSelected.drop('45', axis=1), dfFeatureSelected['45'], test_size=0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "fIqfXSFLPGXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression <a name=\"fifth\"></a>\n"
      ],
      "metadata": {
        "id": "H7vZ6fVtOX5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tune the hyperparameters of the logistic regression model"
      ],
      "metadata": {
        "id": "k6HUMFQ1Sx4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Choose a performance metric\n",
        "scoringMetric = 'accuracy'\n",
        "\n",
        "# Select a range of values for the regularization parameter\n",
        "regParams = np.logspace(-4, 4, 9)\n",
        "\n",
        "# Loop over the regularization parameter values\n",
        "bestScore = 0\n",
        "bestRegParam = None\n",
        "for regParam in regParams:\n",
        "    # Create a logistic regression model with L1 or L2 regularization\n",
        "    if regParam == 0:\n",
        "        l1Regularization = False\n",
        "    else:\n",
        "        l1Regularization = True\n",
        "    if l1Regularization:\n",
        "        model = LogisticRegression(penalty='l1', C=regParam, solver='liblinear')\n",
        "    else:\n",
        "        model = LogisticRegression(penalty='l2', C=1/regParam, solver='lbfgs')\n",
        "    \n",
        "    # Fit the model on the training data\n",
        "    model.fit(xTrain, yTrain)\n",
        "    \n",
        "    # Evaluate the model on the testing data\n",
        "    YPred = model.predict(xTest)\n",
        "    score = accuracy_score(yTest, YPred)\n",
        "    \n",
        "    # Update the best score and regularization parameter\n",
        "    if score > bestScore:\n",
        "        bestScore = score\n",
        "        bestRegParam = regParam\n",
        "\n",
        "# Print the best regularization parameter and score\n",
        "print('Best regularization parameter:', bestRegParam)\n",
        "print('Best', scoringMetric, 'score:', bestScore)\n"
      ],
      "metadata": {
        "id": "lqPnjL1oqSkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the logistic regression model on optimized hyperparameters"
      ],
      "metadata": {
        "id": "PnFzO-JeylUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a logistic regression object\n",
        "logReg = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
        "\n",
        "\n",
        "# train the model on the training data\n",
        "logReg.fit(xTrain, yTrain)\n",
        "\n",
        "# make predictions on the testing data\n",
        "logRegpredictions = logReg.predict(xTest)\n",
        "\n",
        "# evaluate the model performance on test data\n",
        "logRegAccuracy = accuracy_score(yTest, logRegpredictions)\n",
        "logRegF1score = f1_score(yTest, logRegpredictions)\n",
        "logRegconfusionMatrix = confusion_matrix(yTest, logRegpredictions)\n",
        "\n",
        "# make predictions on the training data\n",
        "logRegpredictionsTrain = logReg.predict(xTrain)\n",
        "\n",
        "#evaluate on train data \n",
        "logRegAccuracyTrain = accuracy_score(yTrain, logRegpredictionsTrain)\n",
        "logRegF1scoreTrain = f1_score(yTrain, logRegpredictionsTrain)\n",
        "logRegconfusionMatrix = confusion_matrix(yTrain, logRegpredictionsTrain)\n",
        "\n",
        "print(\"TestAccuracy:\", logRegAccuracy)\n",
        "print(\"Test F1 score:\", logRegF1score)\n",
        "print(\"Test Confusion matrix:\", logRegconfusionMatrix)\n",
        "\n",
        "print(\"Train Accuracy:\", logRegAccuracyTrain)\n",
        "print(\"Train F1 score:\", logRegF1scoreTrain)\n",
        "print(\"Train Confusion matrix:\", logRegpredictionsTrain)\n"
      ],
      "metadata": {
        "id": "gismphPePCk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest <a name=\"sixth\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "4-8W8ByJP-Dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tune the hyperparameters of the random forest model"
      ],
      "metadata": {
        "id": "YdhnLZiAOTCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tune random forest classifier number of trees by plotting a oob error rate per tree number\n",
        "from collections import OrderedDict\n",
        "ensemble_clfs = [\n",
        "    (\n",
        "        \"RandomForestClassifier\",\n",
        "        RandomForestClassifier(\n",
        "            warm_start=True,\n",
        "            oob_score=True,\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
        "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
        "\n",
        "# Range of `n_estimators` values to explore.\n",
        "minTreeNum = 2\n",
        "maxTreeNum = 250\n",
        "\n",
        "for label, clf in ensemble_clfs:\n",
        "    for i in range(minTreeNum, maxTreeNum+1 , 2):\n",
        "        #print(i)\n",
        "        clf.set_params(n_estimators=i, random_state=0)\n",
        "        clf.fit(xTrain, yTrain)\n",
        "\n",
        "        # Record the OOB error for each `n_estimators=i` setting.\n",
        "        oob_error = 1 - clf.oob_score_\n",
        "        #print(oob_error)\n",
        "        error_rate[label].append((i, oob_error))\n",
        "        \n",
        "\n",
        "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
        "for label, clf_err in error_rate.items():\n",
        "    xs, ys = zip(*clf_err)\n",
        "    plt.plot(xs, ys, label=label)\n",
        "\n",
        "\n",
        "idx = np.argmin(ys)\n",
        "xMin = xs[idx]\n",
        "\n",
        "plt.axvline(xMin, color='r', linestyle='--')\n",
        "\n",
        "\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"OOB error rate\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Optimal number of tree:\", xMin)\n"
      ],
      "metadata": {
        "id": "IAjdPm9zQET0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train random forest model with optimized hyperparameters"
      ],
      "metadata": {
        "id": "oBgJSZy9OX6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=198, random_state=42)\n",
        "rfc.fit(xTrain, yTrain)\n",
        "\n",
        "# make predictions on testing data\n",
        "rfPrediction = rfc.predict(xTest)\n",
        "\n",
        "#print out results\n",
        "rfAccuracy = accuracy_score(yTest, rfPrediction)\n",
        "rfF1score = f1_score(yTest, rfPrediction)\n",
        "rfConfusionMatrix = confusion_matrix(yTest, rfPrediction)\n",
        "\n",
        "# make predictions on the training data\n",
        "rfPredictionsTrain = rfc.predict(xTrain)\n",
        "\n",
        "#evaluate on train data \n",
        "rfAccuracyTrain = accuracy_score(yTrain, rfPredictionsTrain)\n",
        "rfF1scoreTrain = f1_score(yTrain, rfPredictionsTrain)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", rfAccuracy)\n",
        "print(\"F1 score:\", rfF1score)\n",
        "print(\"Confusion matrix:\", rfConfusionMatrix)"
      ],
      "metadata": {
        "id": "_eUD-gyQYENS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree <a name=\"seventh\"></a>"
      ],
      "metadata": {
        "id": "4XfMMuVlRAMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tune the hyperparameters of decision tree model"
      ],
      "metadata": {
        "id": "X6FWAMTuPKjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define the hyperparameters to tune and their distribution\n",
        "param_dist = {'max_depth': [3, 5, 7, None],\n",
        "              'min_samples_split': randint(2, 11)}\n",
        "\n",
        "# Create the decision tree classifier model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Use random search to find the best hyperparameters\n",
        "random_search = RandomizedSearchCV(dt, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
        "random_search.fit(xTrain, yTrain)\n",
        "\n",
        "# Print the best hyperparameters and evaluation score\n",
        "print(\"Best hyperparameters: \", random_search.best_params_)\n",
        "print(\"Best evaluation score: \", random_search.best_score_)\n"
      ],
      "metadata": {
        "id": "3fQjhG2Ju4FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train decision tree model using optimized hyperparameters"
      ],
      "metadata": {
        "id": "58MWFfg5PPQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run decision tree using tuned parameters\n",
        "dtc = DecisionTreeClassifier(max_depth=3, min_samples_split=9, random_state=42)\n",
        "dtc.fit(xTrain, yTrain)\n",
        "\n",
        "decisionTreePrediction = dtc.predict(xTest)\n",
        "\n",
        "decisionTreeAccuracy = accuracy_score(yTest, decisionTreePrediction)\n",
        "decisionTreeF1score = f1_score(yTest, decisionTreePrediction)\n",
        "decisionTreeConfusionMatrix = confusion_matrix(yTest, decisionTreePrediction)\n",
        "\n",
        "\n",
        "# make predictions on the training data\n",
        "decisionTreePredictionTrain = dtc.predict(xTrain)\n",
        "\n",
        "#evaluate on train data \n",
        "decisionTreeAccuracyTrain = accuracy_score(yTrain, decisionTreePredictionTrain)\n",
        "decisionTreeF1scoreTrain = f1_score(yTrain, decisionTreePredictionTrain)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", decisionTreeAccuracy)\n",
        "print(\"F1 score:\", decisionTreeF1score)\n",
        "print(\"Confusion matrix:\", decisionTreeConfusionMatrix)"
      ],
      "metadata": {
        "id": "NpI-mLUGRCuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Results (Comparing models) <a name=\"eigth\"></a>\n"
      ],
      "metadata": {
        "id": "t76hOgF02scz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compare accuracies and F1 scores on test dataset between all three models "
      ],
      "metadata": {
        "id": "q6su6JlOPeNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the metrics for the three models\n",
        "model_names = ['Decision Tree', 'Logistic Regression', 'Random Forest']\n",
        "accuracies = [decisionTreeAccuracy, logRegAccuracy, rfAccuracy, ]\n",
        "f1_scores = [decisionTreeF1score, logRegF1score, rfF1score]\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.35\n",
        "\n",
        "# Create an array of indices for the x-axis\n",
        "indices = np.arange(len(model_names))\n",
        "\n",
        "# Create a bar graph for accuracies and F1 scores on test data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(indices, accuracies, bar_width, color='#1f77b4', label='Accuracy')\n",
        "plt.bar(indices + bar_width, f1_scores, bar_width, color='#ff7f0e', label='F1 Score')\n",
        "plt.title('Model Results on Test Data')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(indices + bar_width / 2, model_names)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1COn3e7j256w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compare accuracies and F1 scores on training dataset between all three models "
      ],
      "metadata": {
        "id": "AknGyRpePrNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the metrics for the three models\n",
        "model_names = ['Decision Tree', 'Logistic Regression', 'Random Forest']\n",
        "accuracies = [decisionTreeAccuracyTrain, logRegAccuracyTrain , rfAccuracyTrain]\n",
        "f1_scores = [decisionTreeF1scoreTrain, logRegF1scoreTrain, rfF1scoreTrain]\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.35\n",
        "\n",
        "# Create an array of indices for the x-axis\n",
        "indices = np.arange(len(model_names))\n",
        "\n",
        "# Create a bar graph for accuracies and F1 scores on training data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(indices, accuracies, bar_width, color='#1f77b4', label='Accuracy')\n",
        "plt.bar(indices + bar_width, f1_scores, bar_width, color='#ff7f0e', label='F1 Score')\n",
        "plt.title('Model Results on Train Data')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(indices + bar_width / 2, model_names)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tEy2IMqB3hE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Importance <a name=\"ninth\"></a>\n",
        "\n",
        "\n",
        "In this section, we will be exploring the feature importance rankings given by different models."
      ],
      "metadata": {
        "id": "HwA8GP1FRIRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Features\n",
        "\n",
        "Plotting feature importance rankings with all features"
      ],
      "metadata": {
        "id": "zXx6qjOER_ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the logistic regression model\n",
        "coefs = logReg.coef_[0]\n",
        "features = xTest.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "coefDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': coefs})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "coefDataFramePlot = coefDataFramePlot.reindex(coefDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = coefDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red','blue', 'green']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(coefDataFramePlot['Feature'], coefDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(coefDataFramePlot)):\n",
        "    if coefDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(coefDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Have you ever sought treatment for a mental health issue from a mental health professional?'\n",
        "        elif idx == 1:\n",
        "            label = 'Have you had a mental health disorder in the past?'\n",
        "        else:\n",
        "            label = 'Have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?'\n",
        "        ax.bar(coefDataFramePlot['Feature'][i], coefDataFramePlot['Coefficient'][i], color= colors[idx], label=label)\n",
        "\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Coefficient',fontsize = 20)\n",
        "ax.set_title('Logistic Regression: Feature Importance Plot', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VicOp7DASJTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the random forest model\n",
        "rfcFeatureImportance = rfc.feature_importances_\n",
        "features = xTest.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "rfcDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(rfcFeatureImportance)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "rfcDataFramePlot = rfcDataFramePlot.reindex(rfcDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = rfcDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(rfcDataFramePlot['Feature'], rfcDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(rfcDataFramePlot)):\n",
        "    if rfcDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(rfcDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Have you had a mental health disorder in the past?'\n",
        "        elif idx == 1:\n",
        "            label = 'Have you ever sought treatment for a mental health issue from a mental health professional?'\n",
        "        else:\n",
        "            label = 'What is your age?'\n",
        "        ax.bar(rfcDataFramePlot['Feature'][i], rfcDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Feature Importance',fontsize = 20)\n",
        "ax.set_title('Random Forest: Feature Importance Plot', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "44VTcrRFK-rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the decision tree classifier model\n",
        "dtcFeatureImportance = dtc.feature_importances_\n",
        "features = xTest.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "dtcDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(dtcFeatureImportance)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "dtcDataFramePlot = dtcDataFramePlot.reindex(dtcDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = dtcDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(dtcDataFramePlot['Feature'], dtcDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(dtcDataFramePlot)):\n",
        "    if dtcDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(dtcDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Have you had a mental health disorder in the past?'\n",
        "        elif idx == 1:\n",
        "            label = 'Have you ever sought treatment for a mental health issue from a mental health professional?'\n",
        "        else:\n",
        "            label = 'What country do you live in?'\n",
        "        ax.bar(dtcDataFramePlot['Feature'][i], dtcDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize=20)\n",
        "ax.set_ylabel('Feature Importance', fontsize=20)\n",
        "ax.set_title('Decision Tree: Feature Importance Plot', fontsize=30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IBvfAm8NDKi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Work Logistic Features\n",
        "\n",
        "Plotting feature importance rankings, but only with work logistic related features "
      ],
      "metadata": {
        "id": "Xd039TpESWzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the data\n",
        "# define the list of columns to select for worklogistics\n",
        "workLogisticColumn = [ '5', '6', '8', '10', '26', '28', '41']\n",
        "\n",
        "# use .loc to select only the columns in column to select\n",
        "dfWorkLogistic = dfFeatureSelected.loc[:, workLogisticColumn]\n"
      ],
      "metadata": {
        "id": "_0hlZ8LfM335"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "Vrxss8GAa_uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a logistic regression object\n",
        "logRegWorkLogistic = LogisticRegression()\n",
        "\n",
        "# train the model on the training data\n",
        "logRegWorkLogistic.fit(dfWorkLogistic, dfCombinedVectorized['45'])\n"
      ],
      "metadata": {
        "id": "-XqKEkJuSe44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the logistic regression model\n",
        "coefs = logRegWorkLogistic.coef_[0]\n",
        "features = dfWorkLogistic.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "coefDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': coefs})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "coefDataFramePlot = coefDataFramePlot.reindex(coefDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = coefDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(coefDataFramePlot['Feature'], coefDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(coefDataFramePlot)):\n",
        "    if coefDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(coefDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?'\n",
        "        elif idx == 1:\n",
        "            label = 'Does your employer provide mental health benefits as part of healthcare coverage?'\n",
        "        else:\n",
        "            label = 'Do you know the options for mental health care available under your employer-provided coverage?'\n",
        "        ax.bar(coefDataFramePlot['Feature'][i], coefDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Coefficient',fontsize = 20)\n",
        "ax.set_title('Logistic Regression: Feature Importance Plot (Work Logistic Features)', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "-eJEs3FFZe5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "0RQmK2nQbgVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a random forest object\n",
        "randomForestWorkLogistic = RandomForestClassifier(n_estimators=198, random_state=42)\n",
        "\n",
        "# train the model on the training data\n",
        "randomForestWorkLogistic.fit(dfWorkLogistic, dfCombinedVectorized['45'])\n",
        "\n"
      ],
      "metadata": {
        "id": "Srn-YA5NZe-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the random forest model\n",
        "rfcFeatureImportance = randomForestWorkLogistic.feature_importances_\n",
        "features = dfWorkLogistic.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "rfcDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(rfcFeatureImportance)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "rfcDataFramePlot = rfcDataFramePlot.reindex(rfcDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = rfcDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(rfcDataFramePlot['Feature'], rfcDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(rfcDataFramePlot)):\n",
        "    if rfcDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(rfcDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'If a mental health issue prompted you to request a medical leave from work, asking for that leave would be:'\n",
        "        elif idx == 1:\n",
        "            label = 'Have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?'\n",
        "        else:\n",
        "            label = 'Have your previous employers provided mental health benefits?'\n",
        "        ax.bar(rfcDataFramePlot['Feature'][i], rfcDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Feature Importance',fontsize = 20)\n",
        "ax.set_title('Random Forest: Feature Importance Plot (Work Logistic Features)', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "weVA8dcUby5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree"
      ],
      "metadata": {
        "id": "sMr2FWtxcIBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a decision tree object\n",
        "decisionTreeWorkLogistic = DecisionTreeClassifier(max_depth=3, min_samples_split=9, random_state=42)\n",
        "\n",
        "# train the model on the training data\n",
        "decisionTreeWorkLogistic.fit(dfWorkLogistic, dfCombinedVectorized['45'])\n",
        "\n"
      ],
      "metadata": {
        "id": "5nOwNxfUd4DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the logistic regression model\n",
        "dtcFeatureImportance = decisionTreeWorkLogistic.feature_importances_\n",
        "features = dfWorkLogistic.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "dtcDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(dtcFeatureImportance)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "dtcDataFramePlot = dtcDataFramePlot.reindex(dtcDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = dtcDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(dtcDataFramePlot['Feature'], dtcDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(dtcDataFramePlot)):\n",
        "    if dtcDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(dtcDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Do you know the options for mental health care available under your employer-provided coverage?'\n",
        "        elif idx == 1:\n",
        "            label = 'Have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?'\n",
        "        else:\n",
        "            label = 'Have your previous employers provided mental health benefits?'\n",
        "        ax.bar(dtcDataFramePlot['Feature'][i], dtcDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Feature Importance',fontsize = 20)\n",
        "ax.set_title('Decision Tree: Work Logistic Features Importance Plot', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ziqXEb0QfIHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mental Health Medical History Features\n",
        "\n",
        "Plotting feature importance based on medical history features"
      ],
      "metadata": {
        "id": "fsX8od0jSfWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the data\n",
        "# define the list of columns to select for mental health medical history\n",
        "medicalHistoryColumn = [ '43', '44', '50']\n",
        "\n",
        "# use .loc to select only the columns in column to select\n",
        "dfMedicalHistory = dfFeatureSelected.loc[:, medicalHistoryColumn]\n"
      ],
      "metadata": {
        "id": "OrCcV5OaSl9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "A3BXG39xQXim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a logistic regression object\n",
        "logRegMedicalHistory = LogisticRegression()\n",
        "\n",
        "# train the model on the training data\n",
        "logRegMedicalHistory.fit(dfMedicalHistory, dfCombinedVectorized['45'])\n"
      ],
      "metadata": {
        "id": "P25VXkzuQXLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the logistic regression model\n",
        "coefs = logRegMedicalHistory.coef_[0]\n",
        "features = dfMedicalHistory.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "coefDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': coefs})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "coefDataFramePlot = coefDataFramePlot.reindex(coefDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = coefDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(coefDataFramePlot['Feature'], coefDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(coefDataFramePlot)):\n",
        "    if coefDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(coefDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Have you ever sought treatment for a mental health issue from a mental health professional?'\n",
        "        elif idx == 1:\n",
        "            label = 'Have you had a mental health disorder in the past?'\n",
        "        else:\n",
        "            label = 'Do you have a family history of mental illness?'\n",
        "        ax.bar(coefDataFramePlot['Feature'][i], coefDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Coefficient',fontsize = 20)\n",
        "ax.set_title('Logistic Regression: Feature Importance Plot (Medical History Features)', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9IyWHX7aQbFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest"
      ],
      "metadata": {
        "id": "ctWk_eARQ-RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a random forest object\n",
        "randomForestMedicalHistory = RandomForestClassifier(n_estimators=198, random_state=42)\n",
        "\n",
        "# train the model on the training data\n",
        "randomForestMedicalHistory.fit(dfMedicalHistory, dfCombinedVectorized['45'])\n",
        "\n"
      ],
      "metadata": {
        "id": "BNHRzV5jQ_wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the random forest model\n",
        "rfcFeatureImportance = randomForestMedicalHistory.feature_importances_\n",
        "features = dfMedicalHistory.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "rfcDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(rfcFeatureImportance)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "rfcDataFramePlot = rfcDataFramePlot.reindex(rfcDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = rfcDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(rfcDataFramePlot['Feature'], rfcDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(rfcDataFramePlot)):\n",
        "    if rfcDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(rfcDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Have you had a mental health disorder in the past?'\n",
        "        elif idx == 1:\n",
        "            label = 'Have you ever sought treatment for a mental health issue from a mental health professional?'\n",
        "        else:\n",
        "            label = 'Do you have a family history of mental illness?'\n",
        "        ax.bar(rfcDataFramePlot['Feature'][i], rfcDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Feature Importance',fontsize = 20)\n",
        "ax.set_title('Random Forest: Feature Importance Plot (Medical History Features)', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l7SneBpKRB8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree"
      ],
      "metadata": {
        "id": "Dx5Za_INRgLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a decision forest object\n",
        "decisionTreeMedicalHistory = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# train the model on the training data\n",
        "decisionTreeMedicalHistory.fit(dfMedicalHistory, dfCombinedVectorized['45'])\n",
        "\n"
      ],
      "metadata": {
        "id": "FAlRiKZ3RhnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the decision tree model\n",
        "dtcFeatureImportance = decisionTreeMedicalHistory.feature_importances_\n",
        "features = dfMedicalHistory.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "dtcDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(dtcFeatureImportance)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "dtcDataFramePlot = dtcDataFramePlot.reindex(dtcDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = dtcDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(dtcDataFramePlot['Feature'], dtcDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(dtcDataFramePlot)):\n",
        "    if dtcDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(dtcDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Have you had a mental health disorder in the past?'\n",
        "        elif idx == 1:\n",
        "            label = 'Have you ever sought treatment for a mental health issue from a mental health professional?'\n",
        "        else:\n",
        "            label = 'Do you have a family history of mental illness?'\n",
        "        ax.bar(dtcDataFramePlot['Feature'][i], dtcDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Feature Importance',fontsize = 20)\n",
        "ax.set_title('Decision Tree: Feature Importance Plot (Medical History Features)', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y2TeErt6RoYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mental Health Perception Features\n",
        "\n",
        "Plotting feature importance based on perception toward mental health features"
      ],
      "metadata": {
        "id": "uNbC8vz_SmYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the data\n",
        "# define the list of columns to select for mental health perception features\n",
        "perceptionColumn = [ '13','34','40','62']\n",
        "\n",
        "# use .loc to select only the columns in perception column \n",
        "dfPerception = dfFeatureSelected.loc[:, perceptionColumn]\n"
      ],
      "metadata": {
        "id": "QMmcLWLASupc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "KPRIzQ_gSSNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a logistic regression object\n",
        "logRegPerception = LogisticRegression()\n",
        "\n",
        "# train the model on the training data\n",
        "logRegPerception.fit(dfPerception, dfCombinedVectorized['45'])\n"
      ],
      "metadata": {
        "id": "0fGopA93SSBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the logistic regression model\n",
        "coefs = logRegPerception.coef_[0]\n",
        "features = dfPerception.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "coefDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(coefs)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "coefDataFramePlot = coefDataFramePlot.reindex(coefDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = coefDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(coefDataFramePlot['Feature'], coefDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(coefDataFramePlot)):\n",
        "    if coefDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(coefDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'Would you bring up a mental health issue with a potential employer in an interview?'\n",
        "        elif idx == 1:\n",
        "            label = 'How willing would you be to share with friends and family that you have a mental illness?'\n",
        "        else:\n",
        "            label = 'Would you have been willing to discuss a mental health issue with your direct supervisor(s)?'\n",
        "        ax.bar(coefDataFramePlot['Feature'][i], coefDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Absolute Coefficient',fontsize = 20)\n",
        "ax.set_title('Logistic Regression: Feature Importance Plot (Mental Health Perception Features)', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_7ce8gjSYwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest"
      ],
      "metadata": {
        "id": "cqfKLaamTgLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a random forest object\n",
        "randomForestPerception = RandomForestClassifier(n_estimators=198, random_state=42)\n",
        "\n",
        "# train the model on the training data\n",
        "randomForestPerception.fit(dfPerception, dfCombinedVectorized['45'])\n",
        "\n"
      ],
      "metadata": {
        "id": "-oKj1sSrTgLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the random forest model\n",
        "rfcFeatureImportance = randomForestPerception.feature_importances_\n",
        "features = dfPerception.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "rfcDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(rfcFeatureImportance)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "rfcDataFramePlot = rfcDataFramePlot.reindex(rfcDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = rfcDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(rfcDataFramePlot['Feature'], rfcDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(rfcDataFramePlot)):\n",
        "    if rfcDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(rfcDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'How willing would you be to share with friends and family that you have a mental illness?'\n",
        "        elif idx == 1:\n",
        "            label = 'Would you have been willing to discuss a mental health issue with your direct supervisor(s)?'\n",
        "        else:\n",
        "            label = 'Would you feel comfortable discussing a mental health disorder with your coworkers?'\n",
        "        ax.bar(rfcDataFramePlot['Feature'][i], rfcDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Feature Importance',fontsize = 20)\n",
        "ax.set_title('Random Forest: Feature Importance Plot (Mental Health Perception Features)', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "inZvIKkYTgLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree"
      ],
      "metadata": {
        "id": "xtQ3duJuUDkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a decision forest object\n",
        "decisionTreePerception = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# train the model on the training data\n",
        "decisionTreePerception.fit(dfPerception, dfCombinedVectorized['45'])\n",
        "\n"
      ],
      "metadata": {
        "id": "qM3oHIrZUDkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names from the random forest model\n",
        "dtcFeatureImportance = decisionTreePerception.feature_importances_\n",
        "features = dfPerception.columns\n",
        "\n",
        "# Create a pandas dataframe to store the absolute coefficients and feature names\n",
        "dtcDataFramePlot = pd.DataFrame({'Feature': features, 'Coefficient': abs(dtcFeatureImportance)})\n",
        "\n",
        "# Sort the coefficients in descending order of magnitude\n",
        "dtcDataFramePlot = dtcDataFramePlot.reindex(dtcDataFramePlot['Coefficient'].sort_values(ascending=False).index)\n",
        "\n",
        "# Get the top 3 features with the highest absolute coefficients\n",
        "top3Features = dtcDataFramePlot['Feature'][:3].tolist()\n",
        "\n",
        "# Assign colors to the top 3 features\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(dtcDataFramePlot['Feature'], dtcDataFramePlot['Coefficient'], color='grey')\n",
        "for i in range(len(dtcDataFramePlot)):\n",
        "    if dtcDataFramePlot['Feature'][i] in top3Features:\n",
        "        idx = top3Features.index(dtcDataFramePlot['Feature'][i])\n",
        "        if idx == 0:\n",
        "            label = 'How willing would you be to share with friends and family that you have a mental illness?'\n",
        "        elif idx == 1:\n",
        "            label = 'Would you feel comfortable discussing a mental health disorder with your coworkers?'\n",
        "        else:\n",
        "            label = 'Would you have been willing to discuss a mental health issue with your direct supervisor(s)?'\n",
        "        ax.bar(dtcDataFramePlot['Feature'][i], dtcDataFramePlot['Coefficient'][i], color=colors[idx], label=label)\n",
        "ax.set_xlabel('Feature', fontsize = 20)\n",
        "ax.set_ylabel('Feature Importance',fontsize = 20)\n",
        "ax.set_title('Decision Tree: Feature Importance Plot (Perception Features)', fontsize = 30)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MDMjEh3fUDkM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}